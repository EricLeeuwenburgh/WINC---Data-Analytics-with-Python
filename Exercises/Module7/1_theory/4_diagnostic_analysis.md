# Diagnostic Analysis
Diagnostic analysis focusses on explaining phenomena based on historic data. A good example might be a topic like climate change; we have perceived that the climate is behaving differently from what we are used to and so started looking at the past to determine why this happened. Where descriptive statistics are very much concerned with **what** happened, diagnostic analysis dives into the question of **why** something happened. The strategy for answering this why question is different to the what question; rather than just make a grouping of datapoints, we must find a reason for why the datapoints are arranged in the way that they are. This part of analysis is called 'formulating a hypothesis'. Based on our understanding of the world and the data available, we can formulate a theory as to why something has happened and we can then proceed to test it and scrutinize it. The reason that formulating a sound theory before doing analysis is necessary, is tht we have limited resources.

Consider the climate change example, based on our understanding of the atmosphere and its relationship with CO2, ozone layers, and heat trapping, we can formulate a theory that something has changed in the past 100-200 years in regards to how much CO2 we produce. If we were to not take into account our understanding of the world, we might end up looking if there is a relation between the amount of sea turtles in the Carribean and the rise in temperature. If we had to examine each of these tangents, we would never get to answer our question. In statistics this hypothesis usually is flipped around and formulated as the null-hypothesis, meaning the hypothesis in our example might be: **CO2 emissions have no effect on temperature** and our aim would be to figure out if we can reject the null hypothesis beyond reasonable doubt.

## Rejecting the null-hypothesis
When working within a diagnostic analysis project, you generally have one dataset which encompasses an observation, and one dataset that encompasses the data needed to test your theory. In the climate change example, the first dataset would include temperature data - the change that we have observed. The second dataset would be comprised of data for our theory, so if we went with our sea turtle theory, it might be a dataset of sea turtle counts, or in case of the CO2 theory, a dataset with CO2 emissions. We would then find a way to compare these datasets; for our climate change example we could look to the temperature readings per year and the CO2 emissions per year; by having a common denominator of year we can see if there is any relation between the two datasets. In this specific case we would see a very clear relationship between the two if we plotted the values on to a graph of some kind; the trending line would be similar. However, it should be noted that while plotting is a fun and often useful way to analyze data, it is by no means accurate. There are two reasons for this.

First of all, a relationship between points might not be direct. For instance, it could be possible that CO2 increase only has effect on the temperature 5 years after it occurred, in such a case determining the relationship between the datasets could be much harder based on just a visual of the data.

A bigger issue here is that while a clear relationship may be visible, the relationship does not have to be causal. Meaning, the CO2 emissions might not be the reason that the temperature is rising; maybe the emissions are rising because of the increase in temperature, maybe the 2 are caused by something else entirely. We will dive deeper into the implications of this question of cause and effect in a later chapter on drawing conclusions from data, for now just keep in mind that a relationship between 2 datasets does not necessarily mean anything at all.

If there is no reason to doubt that the relationship between the two datasets is causal, which is to say, the relationship is DEFINITELY causal, then at this point you can reject the null hypothesis and claim that CO2 emissions have an effect on temperature. If there is any doubt at all, it is advisable to put this theory to the test and attempt to make a predictive model for your test. More on this in the next chapter.

## Verification
In order to verify the results of a diagnostic analysis, we can create a predictive model that we then test on historic data. So in the example of climate change we might look at the data for 1900-1980 and formulate a predictive model as to how temperature will increase. Then we will predict the following 20 years and put this alongside the actual data to validate that the relationship is accurate. Note: this tells us nothing about whether this relationship is causal. What we establish with diagnostic analysis is that there is a form of correlation between two datasets.

## Automated Analysis
Finding correlation between datasets is something that computers can do quite efficiently. Comparing all combinations of values of a given dataset can be automated and as a result there's a bunch of tools you can use to find correlations between values. We will look at some of those tools later on in the module. For now all you have to know is that correlations by themselves are not worth much; without knowing what the relationship is between two values, there is no sensible conclusion that you can draw. So while this analysis can be automated, it does not necessarily yield good, reliable, or even useful results.